
<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>the nullfactory</title>
	<meta name="description" content="the nullfactory" />
	<meta name="keywords" content="ALM,Azure,Azure Storage Accounts,CDN,Code Access Security,commitizen,conventional-changelog,Deployment,Dynamics CRM,Dynamics CRM Online,Entity Framework,generator-nullfactory-xrm,Git,HtmlAgilityPack,Hyper-V,Log4Net,Miscellaneous,Mobile Development,nodejs,npm,Octopus Deploy,PowerShell,REST,Security,SharePoint,SOAP,SQL Server,SQL Server Data Tools,SQL Server Reporting Services,StyleCop,Team Build,Team Foundation Server,TFS Tips,TFSVC,Virtualization,Visual Studio,Visual Studio Team Services,WCF,Windows,Windows Phone,Yeoman" />

	<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="the nullfactory" Feed" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<link rel="stylesheet" href="/stylesheets/base.css" type="text/css" />
	<link rel="stylesheet" href="http://www.nullfactory.net/stylesheets/github.css" type="text/css" />
	<link href='http://fonts.googleapis.com/css?family=Lato' rel='stylesheet' type='text/css'>
	<link rel="stylesheet" href="http://www.nullfactory.net/stylesheets/octicons.css" type="text/css" />
  <link rel="stylesheet" type="text/css" href="http://www.nullfactory.net/stylesheets/font-awesome/css/font-awesome.min.css" />
	<link href="https://fonts.googleapis.com/css?family=Lato|Open+Sans" rel="stylesheet">
	<link rel="canonical" href="http://nullfactory.net/page5/" />
</head>
<body>
  <!-- header -->
	<div class="header-container">
	  <header>
		  <ul class="nav">
			<!--Change the  URL here if working on an absolute domain-->
			<li><a href="http://www.nullfactory.net"><img src="/images/logo.png" class="logo"/></a></li>
			<li><a href="http://www.nullfactory.net/archive"><span style="margin-right: 6px;"></span>archive</a></li>
			<li><a href="http://www.nullfactory.net/about"><span style="margin-right: 6px;"></span>about</a></li>
		  </ul>
	  </header>
	</div>
  <!-- header -->

  <div class="container">
	
<header>
  <div class="head-inner">
    <!--<h2>&nbsp;</h2>-->
  </div>
</header>
 <div class="listing">
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/06/entity-framework-multiple-context-namespace-collision/">Entity Framework Namespace Collisions When Working with Multiple Contexts</a></h2>
      <p class="post-date">03 Jun 2015

      | <a href="/category/entity-framework" title="Entity Framework">Entity Framework</a>
      | <a href="/category/sql-server" title="SQL Server">SQL Server</a>
</p>

      <p>
		
		<p>I came across the following exception whilst attempting working with a solution that contained a <em>couple of</em> Entity Framework (EF) 6 database contexts.</p>

<pre><code>System.Data.Entity.Core.MetadataException

Schema specified is not valid. Errors: 
The mapping of CLR type to EDM type is ambiguous because multiple CLR types match the EDM type 'Setting'. Previously found CLR type 'SqlHelper.Primary.Setting', newly found CLR type 'SqlHelper.Secondary.Setting'.

at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.LoadAssemblyFromCache(Assembly assembly, Boolean loadReferencedAssemblies, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.ExplicitLoadFromAssembly(Assembly assembly, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.ExplicitLoadFromAssembly(Assembly assembly, ObjectItemCollection collection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly)
at System.Data.Entity.Internal.InternalContext.TryUpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.UpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.GetEntitySetAndBaseTypeForType(Type entityType)
at System.Data.Entity.Internal.Linq.InternalSet`1.Initialize()
at System.Data.Entity.Internal.Linq.InternalSet`1.get_InternalContext()
at System.Data.Entity.Internal.Linq.InternalSet`1.ActOnSet(Action action, EntityState newState, Object entity, String methodName)
at System.Data.Entity.Internal.Linq.InternalSet`1.Add(Object entity)
at System.Data.Entity.DbSet`1.Add(TEntity entity)
at MultiContextConsoleApp.Program.Main(String[] args) in e:\shane\Projects\Orca\Sandbox\MultiContextConsoleApp\MultiContextConsoleApp\Program.cs:line 16
at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args)
at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args)
at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly()
at System.Threading.ThreadHelper.ThreadStart_Context(Object state)
at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)
at System.Threading.ThreadHelper.ThreadStart()
</code></pre>


		<p><a href="/2015/06/entity-framework-multiple-context-namespace-collision/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/06/generate-clean-up-script-drop-objects-sql-database/">Generate a Clean Up Script to Drop All Objects in a SQL Server Database</a></h2>
      <p class="post-date">02 Jun 2015

      | <a href="/category/sql-server" title="SQL Server">SQL Server</a>
</p>

      <p>
		
		<p>I needed a quick and reusable way to drop all SQL server objects from an Azure database. The objective was to have some kind of process to clean up and prep the database before the main deployment is kicked off. And given that I am particularly biased towards using a sql script my search for a solution focused around it.</p>

<p>In addition to actually dropping the artifacts, the script should be aware of the order in which it should do it - that is to drop the most dependent objects first and work its way towards the least dependent ones. And my nice-to-have feature is to be able to parameterize the schema name so that it could be used with a multi-tenant database schema.</p>

<p>I saw a few possible solutions and finally settled on using the <a href="http://stackoverflow.com/questions/536350/drop-all-the-tables-stored-procedures-triggers-constraints-and-all-the-depend">out-of-the-box feature</a> that's already available through SQL Server Management Studio (SSMS).</p>

<ol>
<li>Open up SQL Server Management Studio.</li>
<li><p>Select <code>Task  &gt; Generate Script...</code> on on your the database context menu. This would open up the <code>Generate and Publish Scripts</code> dialog.</p>

<p><img src="/images/posts/GenerateDropScript/10_ContextMenu.png" alt="SSMS Context Menu" /></p></li>
<li><p>First, navigate to the <code>Choose Objects</code> tab and select all the objects that need to be dropped.</p></li>
<li><p>Next, on the <code>Set Scripting Options</code> tab, select the preferred output location.</p>

<p><img src="/images/posts/GenerateDropScript/20_SetScriptingOptions.png" alt="Set Scripting Options" /></p></li>
<li><p>Next, click the <code>Advanced</code> button which result in the <code>Advanced Scripting Options</code> dialog.</p>

<p><img src="/images/posts/GenerateDropScript/30_AdvancedScriptingOptions.png" alt="Advanced Scripting Options" /></p></li>
<li><p>Navigate down towards to and change the <code>General &gt; Script DROP and CREATE</code> option to <code>Script DROP</code>.</p></li>
<li>Set the default values for the rest of the steps and finally click the <code>Finish</code> button.</li>
</ol>


		<p><a href="/2015/06/generate-clean-up-script-drop-objects-sql-database/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/05/recording-diagnostics-azure-app-service-website-log4net/">Recording Diagnostics on a Azure App Service Hosted Website using Log4Net</a></h2>
      <p class="post-date">24 May 2015

      | <a href="/category/azure" title="Azure">Azure</a>
      | <a href="/category/log4net" title="Log4Net">Log4Net</a>
</p>

      <p>
		
		<p>I've been working on moving an existing web based software solution into the Azure cloud ecosystem. The solution is tightly integrated with and uses Log4Net as it logging framework. My primary goal, in terms of logging, was to keep as much of my original architecture intact and at the same time make maximum use of the diagnostics infrastructure that is available in Azure.</p>

<p>The <a href="http://azure.microsoft.com/en-in/documentation/articles/web-sites-enable-diagnostic-log/">official documentation states</a> that calls to the <code>System.Diagnostics.Trace</code> methods are all that is required to start capturing diagnostic information. In summary, this is all I needed to do:</p>

<ol>
<li>Enable diagnostics and configure the storage locations (discussed later down the post).</li>
<li>From within my code write the <code>Warning</code>, <code>Error</code> and <code>Information</code> messages via their respective trace methods.</li>
<li>...</li>
<li>Azure starts capturing the custom diagnostics information - PROFIT! </li>
</ol>

<p>Sounds simple enough. </p>

<p>So I thought if I just set up a <code>TraceAppender</code> everything would work fine and that would be the end of it. The results were not what I was expecting and this was the output in my table storage:</p>

<p><img src="/images/posts/AzureAppSvcDiag/10_AppServiceTableDiag.png" alt="Table Diagnostics" /></p>

<p>The trace entries are bunched together as a single <code>Verbose</code> entry and the writes appear to be buffered. Not acceptable. I suppose the buffering could be because I had not used the <code>ImmediateFlush</code> option for the <code>TraceAppender</code>, but I need to have each Trace statement to have its own entry in the table.</p>

<p>While there are a lot of posts on the internet on how to setup Log4Net with Azure, most of them appear to be out of date and seem to be compensating for features were not available in the Azure at the time of their implementation. Then there are others that targeted towards integrating with the Cloud Service which is not what I was looking for.</p>


		<p><a href="/2015/05/recording-diagnostics-azure-app-service-website-log4net/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/05/start-azure-webjobs-on-demand/">Start Azure Web Jobs On Demand</a></h2>
      <p class="post-date">18 May 2015

      | <a href="/category/azure" title="Azure">Azure</a>
      | <a href="/category/powershell" title="PowerShell">PowerShell</a>
      | <a href="/category/rest" title="REST">REST</a>
</p>

      <p>
		
		<p>I've been working on an Azure based solution recently and have been using the free tiers to quickly get the solution up and running and to perform the first few QA cycles. The core solution is based around single app service website and then a second website that acts as the host for a continuous web job which is triggered via a queue.
The problem with the free tiers is that there's a high possibility that the web job would shut itself down and hibernate if it is <a href="http://azure.microsoft.com/en-us/documentation/articles/web-sites-create-web-jobs/">idle for more that 20 mins</a>:</p>

<blockquote>
  <p>As of March 2014, web apps in Free mode can time out after 20 minutes if there are no requests to the scm (deployment) site and the web app's portal is not open in Azure. Requests to the actual site will not reset this.</p>
</blockquote>

<p>A little research shows a few possible solutions:</p>

<ol>
<li>If the job is not time sensitive, then manually start the service remotely using a script or tool. </li>
<li>Make your code explicitly start the web job just as a new request is being enqueued. This can be done by <a href="https://github.com/projectkudu/kudu/wiki/WebJobs-API">making a REST call</a> to the deployment site.</li>
<li>And lastly, upgrade to a basic or standard tier and enabling "Always On" keeps the site (and jobs) "warm" and prevent them from hibernating.</li>
</ol>


		<p><a href="/2015/05/start-azure-webjobs-on-demand/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/05/ssdt-deployment-plan-modifer/">Parameterize Schema Name within a SSDT Database Project for Multi-Tenant Solutions</a></h2>
      <p class="post-date">09 May 2015

      | <a href="/category/sql-server" title="SQL Server">SQL Server</a>
      | <a href="/category/sql-server-data-tools" title="SQL Server Data Tools">SQL Server Data Tools</a>
</p>

      <p>
		
		<p>I've been working on an multi-tenant solution recently and have been trying to come up with an efficient way to manage the database deployment and upgrade. The database is designed to segregate each tenant's data under its own schema namespace as such I need to generate a re-useable script that can be deployed against each tenant. The approach I am going to take is to first source control the database schema within a SQL Server Data Tools (SSDT) database project and then use it to generate the script that can be parameterized with the tenant information.</p>

<p>I first parameterized the the schema name as a SQLCMD variable - <code>$TenantName</code>: </p>

<p><img src="/images/posts/DeploymentPlanModifer/10_varcmd.png" alt="SqlCmd Variable" /></p>

<p>Next I tried to replace the schema name with the new variable, but this did not work as trying to build the solution now returns with a 71502 error as the project is no longer able to resolve and validate schema objects.</p>

<p><img src="/images/posts/DeploymentPlanModifer/20_SchemaValidation.png" alt="Schema Validation" /></p>

<p>SQLCMD does not have any complaints if I replace the <code>[dbo].</code> with <code>[$TenantName]</code> in the generated script so its the SSDT project that is attempting to maintain the integrity of database. </p>

<p>One possible way to overcome this is to <a href="http://stackoverflow.com/questions/10826014/suppress-some-warnings-in-sql-server-ssdt">suppress the 71502</a> by turning them into <a href="https://social.msdn.microsoft.com/Forums/sqlserver/en-US/9b698de1-9f6d-4e51-8c73-93c57355e768/treat-specific-warning-as-error?forum=ssdt">warnings</a>. The disadvantage in this approach is that you loose the rich validation in exchange for something that is essentially a deployment convenience.</p>

<p>Another duct tape and bubble gum approach would be to just have some kind of post deployment operation that does a find and replace on the schema name. Sure it would work, but that's not going to be reliable in the long run. </p>

<p>A little bit of research reveals that the proper way to alter the creation of deployment script process is to create a deployment plan modifier. A deployment plan modifier is essentially a class that inherits <code>DeploymentPlanModifier</code> and allows you to inject custom actions when deploying a SQL project. There does not seem to be much formal documentation on the process, so I relied a lot on <a href="https://msdn.microsoft.com/en-US/library/dn306642(v=vs.103).aspx">this article in MSDN</a>, the sample <a href="https://github.com/Microsoft/DACExtensions">DACExtensions</a> and what forum posts I could find. So with a lot of trial and error I wrote my own plan modifier that would replace the schema identifiers when the database project is published.</p>


		<p><a href="/2015/05/ssdt-deployment-plan-modifer/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/03/disabling-sharepoint-search-service/">Disabling the SharePoint Search Service</a></h2>
      <p class="post-date">11 Mar 2015

      | <a href="/category/sharepoint" title="SharePoint">SharePoint</a>
      | <a href="/category/windows" title="Windows">Windows</a>
</p>

      <p>
		
		<p>If left unchecked, the SharePoint search service to attempts to consume most of the available memory in a resource constrained development box. I usually disable it when I am not actively working with it.</p>

<p><a href="http://www.falconitservices.com/support/KB/Lists/Posts/Post.aspx?ID=182">This seems to be most effective</a> way to completely disable the service. Although changing the password to an invalid one works as expected, it is important to replace the user account from a domain to a local user account as well. If not, the constant invalid login attempts could trigger your account lock out threshold policy in the domain.</p>

<p>So for convenience sake I created two batch files - one to disable the service and another to bring it back up.  </p>


		<p><a href="/2015/03/disabling-sharepoint-search-service/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/03/create-elevated-command-prompt-different-user-context/">Create an Elevated Command Prompt Running Under a Different User Context</a></h2>
      <p class="post-date">07 Mar 2015

      | <a href="/category/security" title="Security">Security</a>
</p>

      <p>
		
		<p>User Account Control (UAC) is a security mechanism that is available in most modern versions of Windows. It restricts the ability to make changes to a computer environment without the explicit consent of an administrator. And as is with most production environments, it is very likely that this feature is already enabled. And just as likely, is the need to execute applications and commands under a different user within an elevated context during application deployment or maintenance. </p>

<p>This is achieved by using the <code>runas</code> command with the <code>/noprofile</code> flag. The command below creates an elevated command line running under the <code>wunder\admin</code> user:</p>

<pre><code>runas /noprofile /user:wunder\admin cmd 
</code></pre>

<p>Now, all commands executed within this new command prompt would be in the same elevated status.</p>


		<p><a href="/2015/03/create-elevated-command-prompt-different-user-context/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/03/creating-a-self-signed-wild-card-ssl-certificate/">Creating a Self-Signed Wild Card SSL Certificate for Your Development Environment</a></h2>
      <p class="post-date">05 Mar 2015

      | <a href="/category/security" title="Security">Security</a>
</p>

      <p>
		
		<p>Secure Socket Layer (SSL) is a security standard used to ensure secure communication between a web server and browser and used in most modern web application. As a developer it is prudent to setup your development environment to closely resemble production as much as possible, including security concerns. However, getting a full fledged CA SSL certificate for you development environment might not be the most cost-effective solution. Therefore post summarizes the steps I take to create a self signed wild card certificate to be used in the internal environments. My guide is based on this <a href="https://www.macaw.nl/weblog/2013/6/configuring-an-asp-net-project-for-development-with-ssl">excellent post</a>.</p>

<h2>Create the Certificate</h2>

<p>In order to create the certificate we would be using the <code>MakeCert.exe</code> tool which can be found at <code>C:\Program Files (x86)\Windows Kits\8.1\bin\x64\</code>. This command creates the certificate and adds it to the logged in user's personal certificate store:</p>

<pre><code>makecert -r -pe -e 01/01/2099 -eku 1.3.6.1.5.5.7.3.1 -ss My -n CN="*.wunder.local" -sky exchange -sp "Microsoft RSA SChannel Cryptographic Provider" -sy 12 -len 2048 
</code></pre>

<p>Some of the notable flags:</p>

<ul>
<li><strong>-r</strong> - Indicates that we're creating a self-signed certificate.</li>
<li><strong>-pe</strong> - Includes the private key in the certificate and makes it exportable.</li>
<li><strong>-e</strong> - The validity period of the certificate.</li>
<li><p><strong>-n</strong> - The subject's certificate name - specify the wildcard url. </p>

<p><img src="/images/posts/SSWildCardSSL/1_CreateCertificate.png" alt="Create Certificate" /></p></li>
</ul>


		<p><a href="/2015/03/creating-a-self-signed-wild-card-ssl-certificate/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/03/could-not-establish-trust-relationship-ssl-tls/">Could Not Establish Trust Relationship for SSL/TLS Secure Channel</a></h2>
      <p class="post-date">02 Mar 2015

      | <a href="/category/security" title="Security">Security</a>
</p>

      <p>
		
		<p>A while back I worked on a project that required me to integrate to a third-party web service. The web service also in development in parallel by the external team and our team was provided a development endpoint that would be used for testing. </p>

<p>The problem was the certificate used in the SSL was the same as the one production. This resulted in any call to the web service throwing an <code>Could not establish trust relationship for SSL/TLS secure channel</code> error because of the url mismatch. </p>

<p>Due to various constraints we were unable to get certificate replaced. So our temporary work around was to make our code to explicitly trust the external web service host:</p>


		<p><a href="/2015/03/could-not-establish-trust-relationship-ssl-tls/">Read more...</a></p>
	  </p>
    </div>
    <div class="post other link">
      <h2><span class="mega-octicon octicon-file-text" style="min-width: 32px;"></span><a href="/2015/02/obfuscate-sharepoint-ssrs-datasource/">Obfuscating a SharePoint-Integrated SSRS DataSource's Connection String</a></h2>
      <p class="post-date">16 Feb 2015

      | <a href="/category/sharepoint" title="SharePoint">SharePoint</a>
      | <a href="/category/sql-server" title="SQL Server">SQL Server</a>
      | <a href="/category/sql-server-reporting-services" title="SQL Server Reporting Services">SQL Server Reporting Services</a>
      | <a href="/category/code-access-security" title="Code Access Security">Code Access Security</a>
</p>

      <p>
		
		<p>When SQL Server Reporting Services (SSRS) is deployed as an SharePoint integrated solution, it enables much of its functionality to be managed right from within SharePoint. Starting from the 2013 version, the integration between SharePoint and SQL Server Reporting Services 2012 is more tightly coupled than previous iterations. </p>

<p>One feature in integrated mode is the ability to have the data sources (.rsds) and report files (.rdl) within a document library itself. This means that reports can reference a DataSource within any document library in the SharePoint site. </p>

<p>In order for the report to work the user should have read permission on both the data source as well as the report file. The problem with this is that the same user can now potentially view the settings within the data source file, including the connection string.</p>

<h2>The Solution</h2>

<p>In order to protect the connection string, I came up with a solution to obscure it through encryption. The solution can be broken down to two major steps:</p>

<ol>
<li>Force the reports to get the connection string by evaluating an expression embedded within itself. </li>
<li>Within this expression, call some custom code which manages the retrieval and decryption of the connection string. </li>
</ol>

<p>One of the limitations with this method is that you can no longer use a shared data source and each report has to have its credentials embedded.</p>

<p>In my example below, I will be retrieving the configuration string from a configuration list stored in the same SharePoint server. </p>


		<p><a href="/2015/02/obfuscate-sharepoint-ssrs-datasource/">Read more...</a></p>
	  </p>
    </div>
</div>

  <div id="post-pagination" class="pagination">


      <p class="previous">
        <a href="/page4">Previous Page</a>
        </p>


    <p class="previous">
      <a href="/page6">Next Page</a>
    </p>

  </div>

  </div>

  <!-- /.main -->

  <script type="text/javascript">
var _gaq = _gaq || [];

_gaq.push(['_setAccount', 'UA-57983893-1']);
_gaq.push(['_trackPageview']);
        
(function () {
    var ga = document.createElement('script');
    ga.type = 'text/javascript';
    ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(ga, s);
})();
</script>
</body>
</html>
