<rss xmlns:a10="http://www.w3.org/2005/Atom" version="2.0"><channel><title>nullfactory.net</title><link>http://nullfactory.net/rss.xml</link><description>nullfactory.net</description><item><guid isPermaLink="true">http://nullfactory.net/2015/12/disabling-recurring-ondemand-webjob/</guid><link>http://nullfactory.net/2015/12/disabling-recurring-ondemand-webjob/</link><title>Disabling Recurring and OnDemand Web Jobs within a Deployment Slot</title><description>&lt;p&gt;I've come to realize that things can get a bit tricky when working with slot deployments and webjobs. I learned &lt;em&gt;the hard way&lt;/em&gt; that stopping a slotted Web App does &lt;strong&gt;not&lt;/strong&gt; stop the web jobs it hosts. This means that I can't just stop all my website slots and expect the related jobs to automatically shutdown as well. Bummer. 
Okay, so I am thinking maybe I'll just disable the individual jobs for each of the slots? Not much luck on that front either as the Azure portal only provides a &lt;code&gt;stop&lt;/code&gt; option for &lt;code&gt;continuous&lt;/code&gt; jobs and not for the &lt;code&gt;recurring&lt;/code&gt; and &lt;code&gt;OnDemand&lt;/code&gt; jobs.&lt;/p&gt;

&lt;p&gt;This limits us to a few possible solutions:&lt;/p&gt;

</description><pubDate>Sat, 19 Dec 2015 18:30:00 Z</pubDate><a10:updated>2015-12-19T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've come to realize that things can get a bit tricky when working with slot deployments and webjobs. I learned &lt;em&gt;the hard way&lt;/em&gt; that stopping a slotted Web App does &lt;strong&gt;not&lt;/strong&gt; stop the web jobs it hosts. This means that I can't just stop all my website slots and expect the related jobs to automatically shutdown as well. Bummer. 
Okay, so I am thinking maybe I'll just disable the individual jobs for each of the slots? Not much luck on that front either as the Azure portal only provides a &lt;code&gt;stop&lt;/code&gt; option for &lt;code&gt;continuous&lt;/code&gt; jobs and not for the &lt;code&gt;recurring&lt;/code&gt; and &lt;code&gt;OnDemand&lt;/code&gt; jobs.&lt;/p&gt;

&lt;p&gt;This limits us to a few possible solutions:&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;ol&gt;
&lt;li&gt;Make sure that all the resources accessed by the slotted web app / jobs are isolated from each other - Ensure that the necessary appsettings are defined as slot settings and point to different values. This is done so as to ensure that you do not inadvertently run a scheduled job multiple times (once via the production slot and one time for each of the slots). &lt;/li&gt;
&lt;li&gt;&lt;p&gt;Baking-in custom disabling logic right into your job - For example, a web job could skip its operation based on a custom appsetting value. Once again, this appsetting would be defined as a slot setting. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A word of caution - be weary when implementing the "skipping" logic on continous jobs as "skipping" can be considered a successful run which would in turn pop the last message in the tiggered queue.&lt;/p&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The nuke option - entirely delete the web job entries in each of the slots (not recommended).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/11/strategy-updating-sql-server-schema-via-ssdt-delta-script/</guid><link>http://nullfactory.net/2015/11/strategy-updating-sql-server-schema-via-ssdt-delta-script/</link><title>Strategy for Updating a SQL Server Database Schema via SSDT Delta Script</title><description>&lt;p&gt;This posts talks about the high level steps I went through in order to get the SQL Server related components ready for automation in a project I worked on recently. &lt;/p&gt;

&lt;p&gt;This project uses SQL Server Data Tools (SSDT) project in order to maintain the database schema in source control. Its output - the Data-tier Application Component Packages (DACPAC) gets deployed into the appropriate target environment via a WebDeploy package. And considering that the solution was designed as an Entity Framework (EF) database first approach, code first migrations were not a viable upgrade strategy.&lt;/p&gt;

&lt;p&gt;Here are the steps I followed in order to bring the production environment up-to-date: &lt;/p&gt;

</description><pubDate>Tue, 24 Nov 2015 18:30:00 Z</pubDate><a10:updated>2015-11-24T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;This posts talks about the high level steps I went through in order to get the SQL Server related components ready for automation in a project I worked on recently. &lt;/p&gt;

&lt;p&gt;This project uses SQL Server Data Tools (SSDT) project in order to maintain the database schema in source control. Its output - the Data-tier Application Component Packages (DACPAC) gets deployed into the appropriate target environment via a WebDeploy package. And considering that the solution was designed as an Entity Framework (EF) database first approach, code first migrations were not a viable upgrade strategy.&lt;/p&gt;

&lt;p&gt;Here are the steps I followed in order to bring the production environment up-to-date: &lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;ol&gt;
&lt;li&gt;Create a baseline DACPAC and move it into source control - this represents the schema currently in production.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, ensure that every time the SSDT project is built a post event would generate a differential delta script between the baseline and latest DACPAC. I tried to simplify the following command by wrapping it up within a powershell script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;amp;"C:\Program Files (x86)\Microsoft SQL Server\110\DAC\bin\sqlpackage" /a:Script /sf:$SourceDacpac  /tf:$TargetDacpac /op:$OutputDeltaFile /tdn:$DBName /p:IncludeTransactionalScripts=True /p:IncludeCompositeObjects=True /p:ScriptDatabaseOptions=False /p:BlockOnPossibleDataLoss=True /v:TenantSchemaName=dbo
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that one of the parameters (&lt;code&gt;p:IncludeTransactionalScripts=True&lt;/code&gt;) was to ensure that the script would be generated as a transaction.  &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(optional) Perform any post processing on the generated delta script - In my specific use-case I had to tinker the script to work within a multi-tenant scenario. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Deploy the generated delta script against the target environment. This can be done using a tools &lt;code&gt;SqlCmd&lt;/code&gt; or a custom tool such as &lt;a href="https://github.com/rusanu/DbUtilSqlCmd"&gt;https://github.com/rusanu/DbUtilSqlCmd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Upon successful release, update the baseline to the latest DACPAC file.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/22352298/sqlpackage-with-script-action-does-not-produce-any-copy-always-scripts"&gt;Stack Overflow - ssdt - SQLPackage with Script Action does not produce any Copy Always scripts&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/15502659/what-is-the-syntax-for-adding-multiple-arguments-onto-the-variables-parameter"&gt;Stack Overflow - ssdt - What is the syntax for adding multiple arguments onto the "Variables" parameter in sqlpackage.exe?&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://msdn.microsoft.com/en-us/library/hh550080(v=vs.103).aspx"&gt;MSDN - SqlPackage.exe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://phoebix.com/2013/09/19/extract-dacpac-using-command-line/"&gt;Extract DacPac Using Command Line | phoebix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/rusanu/DbUtilSqlCmd"&gt;rusanu/DbUtilSqlCmd Â· GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/10/cdn-streaming-video-azure-storage/</guid><link>http://nullfactory.net/2015/10/cdn-streaming-video-azure-storage/</link><title>Setting up a CDN to Stream Video via Azure Storage</title><description>&lt;p&gt;I needed to setup an Azure Content Delivery Network (CDN) in order to stream some video files stored in Azure Blob Storage. Sounds simple enough right? Well, yes for the most part, but I did hit a few hurdles along the way. This post would hopefully help me avoid them the next time.&lt;/p&gt;

&lt;h2&gt;Create the Storage Account&lt;/h2&gt;

&lt;p&gt;Something I found out after the fact was that CDN endpoints &lt;a href="http://stackoverflow.com/questions/32569564/azure-resource-manager-deployment-vs-classic-deployment-of-storage-accounts"&gt;currently only support classic storage accounts&lt;/a&gt;. So the first order of business is to create a classic storage account either via old portal or using a &lt;a href="http://nullfactory.net/2015/10/deploy-classic-storage-azure-resource-manager/"&gt;resource group manager template&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Another thing I found out is that, at the time of writing, classic storage accounts cannot be made under the 'East US' location. The closest alternative was 'East US 2' and worked fine; I guess its something worth considering if you wanted to co-locate all your resources.&lt;/p&gt;

&lt;p&gt;Next, create a container within storage account - the container would host the files that would be served by the CDN. It can be created manually via the old portal or even through visual studio. Ensure that container access type is set to &lt;code&gt;Public Blob&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Upgrade the Storage Account to a Newer Service Version&lt;/h2&gt;

&lt;p&gt;The first time I tried to tried to stream a video, it did not work as expected; stream was very choppy. It turns out that the service version that got set on the storage was not the latest. &lt;a href="http://blog.thoughtstuff.co.uk/2014/01/streaming-mp4-video-files-in-azure-storage-containers-blob-storage/"&gt;Read more here&lt;/a&gt;, &lt;a href="https://msdn.microsoft.com/library/azure/dd894041.aspx"&gt;and here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So the next step is update the storage account to the latest version in order to take advantage of the improvements. This can be done using the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    var credentials = new StorageCredentials("accountname", "accountkey");
    var account = new CloudStorageAccount(credentials, true);
    var client = account.CreateCloudBlobClient();
    var properties = client.GetServiceProperties();
    properties.DefaultServiceVersion = "2013-08-15";
    client.SetServiceProperties(properties);
    Console.WriteLine(properties.DefaultServiceVersion);
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Sat, 10 Oct 2015 18:30:00 Z</pubDate><a10:updated>2015-10-10T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I needed to setup an Azure Content Delivery Network (CDN) in order to stream some video files stored in Azure Blob Storage. Sounds simple enough right? Well, yes for the most part, but I did hit a few hurdles along the way. This post would hopefully help me avoid them the next time.&lt;/p&gt;

&lt;h2&gt;Create the Storage Account&lt;/h2&gt;

&lt;p&gt;Something I found out after the fact was that CDN endpoints &lt;a href="http://stackoverflow.com/questions/32569564/azure-resource-manager-deployment-vs-classic-deployment-of-storage-accounts"&gt;currently only support classic storage accounts&lt;/a&gt;. So the first order of business is to create a classic storage account either via old portal or using a &lt;a href="http://nullfactory.net/2015/10/deploy-classic-storage-azure-resource-manager/"&gt;resource group manager template&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Another thing I found out is that, at the time of writing, classic storage accounts cannot be made under the 'East US' location. The closest alternative was 'East US 2' and worked fine; I guess its something worth considering if you wanted to co-locate all your resources.&lt;/p&gt;

&lt;p&gt;Next, create a container within storage account - the container would host the files that would be served by the CDN. It can be created manually via the old portal or even through visual studio. Ensure that container access type is set to &lt;code&gt;Public Blob&lt;/code&gt;.&lt;/p&gt;

&lt;h2&gt;Upgrade the Storage Account to a Newer Service Version&lt;/h2&gt;

&lt;p&gt;The first time I tried to tried to stream a video, it did not work as expected; stream was very choppy. It turns out that the service version that got set on the storage was not the latest. &lt;a href="http://blog.thoughtstuff.co.uk/2014/01/streaming-mp4-video-files-in-azure-storage-containers-blob-storage/"&gt;Read more here&lt;/a&gt;, &lt;a href="https://msdn.microsoft.com/library/azure/dd894041.aspx"&gt;and here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So the next step is update the storage account to the latest version in order to take advantage of the improvements. This can be done using the following code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    var credentials = new StorageCredentials("accountname", "accountkey");
    var account = new CloudStorageAccount(credentials, true);
    var client = account.CreateCloudBlobClient();
    var properties = client.GetServiceProperties();
    properties.DefaultServiceVersion = "2013-08-15";
    client.SetServiceProperties(properties);
    Console.WriteLine(properties.DefaultServiceVersion);
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;Create the CDN Endpoint&lt;/h2&gt;

&lt;p&gt;Setting up the CDN itself it pretty straight forward:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a new CDN through the old portal by selecting &lt;code&gt;New &amp;gt; CDN &amp;gt; Quick Create&lt;/code&gt;. &lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select your subscription and set the origin type as &lt;code&gt;Storage Account&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://nullfactory.net/images/posts/AzureCDNStream/10_CreateCDN.png"&gt;&lt;img src="http://nullfactory.net/images/posts/AzureCDNStream/10_CreateCDN.png" alt="Azure Create CDN" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select one of the classic storage accounts from the &lt;code&gt;Origin Url&lt;/code&gt; drop down and hit the create button.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://nullfactory.net/images/posts/AzureCDNStream/20_CDNCreated.png"&gt;&lt;img src="http://nullfactory.net/images/posts/AzureCDNStream/20_CDNCreated.png" alt="Azure CDN Created" /&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;Upload Content&lt;/h2&gt;

&lt;p&gt;Now that everything is setup, go ahead and upload the content into blob storage using Visual Studio or &lt;a href="https://azurestorageexplorer.codeplex.com/"&gt;Azure Storage Explorer&lt;/a&gt;. Once the content is propagated, video streaming should be smooth and working as expected.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/32569564/azure-resource-manager-deployment-vs-classic-deployment-of-storage-accounts"&gt;Stack Overflow - Azure Resource Manager Deployment vs Classic Deployment of Storage Accounts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/7235082/is-microsoft-azure-cdn-a-real-cdn-or-something-else-entirely"&gt;Stack Overflow - Is Microsoft Azure CDN A Real CDN Or Something Else Entirely?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.thoughtstuff.co.uk/2014/01/streaming-mp4-video-files-in-azure-storage-containers-blob-storage/"&gt;Streaming MP4 video in Azure Storage containers (Blob Storage) | thoughtstuff | Tom Morgan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://msdn.microsoft.com/library/azure/dd894041.aspx"&gt;MSDN - Versioning for the Azure Storage Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://azurestorageexplorer.codeplex.com/"&gt;Azure Storage Explorer - Home&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/10/deploy-classic-storage-azure-resource-manager/</guid><link>http://nullfactory.net/2015/10/deploy-classic-storage-azure-resource-manager/</link><title>Deploying a Azure Classic Storage Account using Azure Resource Manager</title><description>&lt;p&gt;I've been working on Azure Resource Group templates quite a bit over the last few weeks. While it has been a pleasant experience overall, I ran into some hurdles the other day while attempting to figure out how to create a &lt;code&gt;Microsoft.ClassicStorage/StorageAccounts&lt;/code&gt; using the Azure Resource Manager(ARM) API.&lt;/p&gt;

&lt;p&gt;The latest version (2.7 at the time of writing) of Azure SDK GUI tools for visual studio were not particularly helpful in generating the required json, but thankfully &lt;a href="http://stackoverflow.com/questions/27347200/add-storage-to-azure-resource-manager"&gt;this&lt;/a&gt; post pointed me in the right direction. And after a little bit of fiddling I find that &lt;code&gt;2015-06-01&lt;/code&gt; appears to be last supported &lt;code&gt;apiVersion&lt;/code&gt; that works with classic storage.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://nullfactory.net/images/posts/DeployClassicStorageArm/10_SupportedVersion.png"&gt;&lt;img src="http://nullfactory.net/images/posts/DeployClassicStorageArm/10_SupportedVersion.png" alt="Azure PowerShell Unsupported" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here's the final script I used to create a classic storage container: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "PrimaryStorageName": {
            "type": "string"
        },
        "PrimaryStorageType": {
            "type": "string",
            "defaultValue": "Standard_LRS",
            "allowedValues": [
                "Standard_LRS",
                "Standard_GRS",
                "Standard_ZRS"
            ]
        },
        "PrimaryStorageLocation": {
        "type": "string",
        "defaultValue": "East US",
        "allowedValues": [
            "East US",
            "West US",
            "West Europe",
            "East Asia",
            "South East Asia"
            ]
        }
    },
    "variables": {
    },
    "resources": [
        {
            "name": "[parameters('PrimaryStorageName')]",
            "type": "Microsoft.ClassicStorage/StorageAccounts",
            "location": "[parameters('PrimaryStorageLocation')]",
            "apiVersion":  "2015-06-01",
            "dependsOn": [ ],
            "properties": {
                "accountType": "[parameters('PrimaryStorageType')]"
            }
        }
    ],
    "outputs": {
    }
}
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Fri, 09 Oct 2015 18:30:00 Z</pubDate><a10:updated>2015-10-09T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've been working on Azure Resource Group templates quite a bit over the last few weeks. While it has been a pleasant experience overall, I ran into some hurdles the other day while attempting to figure out how to create a &lt;code&gt;Microsoft.ClassicStorage/StorageAccounts&lt;/code&gt; using the Azure Resource Manager(ARM) API.&lt;/p&gt;

&lt;p&gt;The latest version (2.7 at the time of writing) of Azure SDK GUI tools for visual studio were not particularly helpful in generating the required json, but thankfully &lt;a href="http://stackoverflow.com/questions/27347200/add-storage-to-azure-resource-manager"&gt;this&lt;/a&gt; post pointed me in the right direction. And after a little bit of fiddling I find that &lt;code&gt;2015-06-01&lt;/code&gt; appears to be last supported &lt;code&gt;apiVersion&lt;/code&gt; that works with classic storage.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://nullfactory.net/images/posts/DeployClassicStorageArm/10_SupportedVersion.png"&gt;&lt;img src="http://nullfactory.net/images/posts/DeployClassicStorageArm/10_SupportedVersion.png" alt="Azure PowerShell Unsupported" /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here's the final script I used to create a classic storage container: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "PrimaryStorageName": {
            "type": "string"
        },
        "PrimaryStorageType": {
            "type": "string",
            "defaultValue": "Standard_LRS",
            "allowedValues": [
                "Standard_LRS",
                "Standard_GRS",
                "Standard_ZRS"
            ]
        },
        "PrimaryStorageLocation": {
        "type": "string",
        "defaultValue": "East US",
        "allowedValues": [
            "East US",
            "West US",
            "West Europe",
            "East Asia",
            "South East Asia"
            ]
        }
    },
    "variables": {
    },
    "resources": [
        {
            "name": "[parameters('PrimaryStorageName')]",
            "type": "Microsoft.ClassicStorage/StorageAccounts",
            "location": "[parameters('PrimaryStorageLocation')]",
            "apiVersion":  "2015-06-01",
            "dependsOn": [ ],
            "properties": {
                "accountType": "[parameters('PrimaryStorageType')]"
            }
        }
    ],
    "outputs": {
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/documentation/articles/resource-manager-deployment-model/"&gt;Microsoft Azure - Understand differences between Resource Manager and classic deployment models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/27347200/add-storage-to-azure-resource-manager"&gt;Stack Overflow - Add storage to Azure resource manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/31886601/how-to-force-azure-storage-account-as-classic"&gt;Stack Overflow - powershell - How to force Azure Storage Account as classic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/08/publishing-assemblies-without-gacutil/</guid><link>http://nullfactory.net/2015/08/publishing-assemblies-without-gacutil/</link><title>Publishing Assemblies into the GAC without GacUtil</title><description>&lt;p&gt;I constantly find myself googling this code snippet - its nice to keep handy: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[System.Reflection.Assembly]::Load("System.EnterpriseServices, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
$publish = New-Object System.EnterpriseServices.Internal.Publish
$publish.GacInstall("c:\temp\publish_dll.dll")
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Sun, 30 Aug 2015 18:30:00 Z</pubDate><a10:updated>2015-08-30T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I constantly find myself googling this code snippet - its nice to keep handy: &lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[System.Reflection.Assembly]::Load("System.EnterpriseServices, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a")
$publish = New-Object System.EnterpriseServices.Internal.Publish
$publish.GacInstall("c:\temp\publish_dll.dll")
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://msdn.microsoft.com/en-us/library/system.enterpriseservices.internal.publish.gacinstall.aspx"&gt;MSDN - Publish.GacInstall Method (System.EnterpriseServices.Internal)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://weblogs.asp.net/adweigert/powershell-install-gac-gacutil-for-powershell"&gt;The Technical Adventures of Adam Weigert - PowerShell: Install-Gac (GACUTIL for PowerShell)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/24950268/deploy-multiple-dll-files-into-gac-without-gacutil"&gt;StackOverflow.com - Deploy multiple dll files into gac without gacutil&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/08/sharing-configuration-between-webjobs/</guid><link>http://nullfactory.net/2015/08/sharing-configuration-between-webjobs/</link><title>Sharing Configuration Between WebJobs</title><description>&lt;p&gt;The project I am working on started out with the single webjob and has since grown to multiple jobs running in parallel. They are all hosted within a dedicated web app, which allows us to scale the jobs independent of the rest of the application. And because they share a single container there is the added side effect of the jobs sharing all the Application Settings and connection strings too.&lt;/p&gt;

&lt;p&gt;While each webjob had its own class library, I didn't want to maintain multiple copies of  the &lt;code&gt;App.Config&lt;/code&gt; file. I decided to share the the common bits (&lt;code&gt;AppSettings&lt;/code&gt; and &lt;code&gt;ConnectionString&lt;/code&gt; sections) in their own files:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In one of the webjob projects, I moved the &lt;code&gt;AppSettings&lt;/code&gt; and &lt;code&gt;ConnectionStrings&lt;/code&gt; into their own &lt;code&gt;.config&lt;/code&gt; files - &lt;code&gt;appSettings.config&lt;/code&gt; and &lt;code&gt;connectionStrings.config&lt;/code&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, I referenced them back to the &lt;code&gt;App.config&lt;/code&gt; using the &lt;code&gt;configSource&lt;/code&gt; attribute.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, I added the same files as linked files to the otherweb jobs and set their &lt;code&gt;Copy to Output Directory&lt;/code&gt; file property to &lt;code&gt;Copy Always&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This works well enough, but for one caveat - which prompted me to write this post in the first place. The problem is that the &lt;code&gt;Web Deploy Package&lt;/code&gt; publishing process does not appear to honor folder structures for the config files. That means that if you've separated the configuration into sub folders (like shown below), the publishing process would flatten it out.&lt;/p&gt;

</description><pubDate>Fri, 28 Aug 2015 18:30:00 Z</pubDate><a10:updated>2015-08-28T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;The project I am working on started out with the single webjob and has since grown to multiple jobs running in parallel. They are all hosted within a dedicated web app, which allows us to scale the jobs independent of the rest of the application. And because they share a single container there is the added side effect of the jobs sharing all the Application Settings and connection strings too.&lt;/p&gt;

&lt;p&gt;While each webjob had its own class library, I didn't want to maintain multiple copies of  the &lt;code&gt;App.Config&lt;/code&gt; file. I decided to share the the common bits (&lt;code&gt;AppSettings&lt;/code&gt; and &lt;code&gt;ConnectionString&lt;/code&gt; sections) in their own files:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;In one of the webjob projects, I moved the &lt;code&gt;AppSettings&lt;/code&gt; and &lt;code&gt;ConnectionStrings&lt;/code&gt; into their own &lt;code&gt;.config&lt;/code&gt; files - &lt;code&gt;appSettings.config&lt;/code&gt; and &lt;code&gt;connectionStrings.config&lt;/code&gt; respectively.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, I referenced them back to the &lt;code&gt;App.config&lt;/code&gt; using the &lt;code&gt;configSource&lt;/code&gt; attribute.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, I added the same files as linked files to the otherweb jobs and set their &lt;code&gt;Copy to Output Directory&lt;/code&gt; file property to &lt;code&gt;Copy Always&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This works well enough, but for one caveat - which prompted me to write this post in the first place. The problem is that the &lt;code&gt;Web Deploy Package&lt;/code&gt; publishing process does not appear to honor folder structures for the config files. That means that if you've separated the configuration into sub folders (like shown below), the publishing process would flatten it out.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;appSettings configSource="CommonConfig/appSettings.config" /&amp;gt;
&amp;lt;connectionStrings configSource="CommonConfig/connectionString.config" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/ShareConfigWebJob/10_SolutionStructure.png" alt="Solution Structure" /&gt;&lt;/p&gt;

&lt;p&gt;This is the new structure created when published:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/ShareConfigWebJob/20_PackageStructure.png" alt="Package Structure" /&gt;&lt;/p&gt;

&lt;p&gt;I suppose we could possibly make it work if we create the webjob folder structure manually in the App_Data folder in your web project, but I don't think its worth the additional complexity for such a trivial issue.&lt;/p&gt;

&lt;p&gt;I guess the work around/best practice would be use a simple flat configuration folder structure.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/7417062/how-to-use-partial-config-files"&gt;c# - How to use partial config files - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/08/enable-ssrs-remote-error-sharepoint-integrated/</guid><link>http://nullfactory.net/2015/08/enable-ssrs-remote-error-sharepoint-integrated/</link><title>Enable SSRS Remote Errors in SharePoint Integrated Mode</title><description>&lt;p&gt;Any time I have to troubleshoot issues in SQL Server Reporting Services (SSRS) reports in a production environment, I usually end up enabling &lt;code&gt;Remote Errors&lt;/code&gt; at some point as part my process. &lt;/p&gt;

&lt;p&gt;Remote errors are enabled via the SSRS Service Application:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;code&gt;Central Administration &amp;gt; Application Management &amp;gt; Manage Service Applications&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Next, click on the appropriate  &lt;code&gt;SQL Server Reporting Services Service Application&lt;/code&gt; service application to manage it. &lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;System Settings&lt;/code&gt; from the toolbar.&lt;/li&gt;
&lt;li&gt;Finally, enable remote errors by navigating into checking the &lt;code&gt;Enable Remote Errors&lt;/code&gt; checkbox. &lt;/li&gt;
&lt;/ol&gt;

</description><pubDate>Thu, 27 Aug 2015 18:30:00 Z</pubDate><a10:updated>2015-08-27T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;Any time I have to troubleshoot issues in SQL Server Reporting Services (SSRS) reports in a production environment, I usually end up enabling &lt;code&gt;Remote Errors&lt;/code&gt; at some point as part my process. &lt;/p&gt;

&lt;p&gt;Remote errors are enabled via the SSRS Service Application:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;code&gt;Central Administration &amp;gt; Application Management &amp;gt; Manage Service Applications&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Next, click on the appropriate  &lt;code&gt;SQL Server Reporting Services Service Application&lt;/code&gt; service application to manage it. &lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;System Settings&lt;/code&gt; from the toolbar.&lt;/li&gt;
&lt;li&gt;Finally, enable remote errors by navigating into checking the &lt;code&gt;Enable Remote Errors&lt;/code&gt; checkbox. &lt;/li&gt;
&lt;/ol&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Although the &lt;a href="https://msdn.microsoft.com/en-us/library/aa337165.aspx"&gt;MSDN documentation&lt;/a&gt; states that this is all that is required, I've found that I need to enable it on the individual site's settings in order to get it working:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Navigate to &lt;code&gt;Site Settings &amp;gt; Reporting Services &amp;gt; Reporting Services Site Settings&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Ensure that the &lt;code&gt;Enable Local Mode Error Messages&lt;/code&gt; is checked.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the troubleshooting session is over, follow the same steps in reverse order to disable remote errors.  &lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://msdn.microsoft.com/en-us/library/aa337165.aspx"&gt;Enable Remote Errors (Reporting Services)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/4850346/ssrs-remote-errors-enabled-but-not-working"&gt;StackOverflow - SSRS Remote Errors Enabled but NOT Working&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/06/entity-framework-multiple-context-namespace-collision/</guid><link>http://nullfactory.net/2015/06/entity-framework-multiple-context-namespace-collision/</link><title>Entity Framework Namespace Collisions When Working with Multiple Contexts</title><description>&lt;p&gt;I came across the following exception whilst attempting working with a solution that contained a &lt;em&gt;couple of&lt;/em&gt; Entity Framework (EF) 6 database contexts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;System.Data.Entity.Core.MetadataException

Schema specified is not valid. Errors: 
The mapping of CLR type to EDM type is ambiguous because multiple CLR types match the EDM type 'Setting'. Previously found CLR type 'SqlHelper.Primary.Setting', newly found CLR type 'SqlHelper.Secondary.Setting'.

at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.LoadAssemblyFromCache(Assembly assembly, Boolean loadReferencedAssemblies, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.ExplicitLoadFromAssembly(Assembly assembly, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.ExplicitLoadFromAssembly(Assembly assembly, ObjectItemCollection collection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly)
at System.Data.Entity.Internal.InternalContext.TryUpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.UpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.GetEntitySetAndBaseTypeForType(Type entityType)
at System.Data.Entity.Internal.Linq.InternalSet`1.Initialize()
at System.Data.Entity.Internal.Linq.InternalSet`1.get_InternalContext()
at System.Data.Entity.Internal.Linq.InternalSet`1.ActOnSet(Action action, EntityState newState, Object entity, String methodName)
at System.Data.Entity.Internal.Linq.InternalSet`1.Add(Object entity)
at System.Data.Entity.DbSet`1.Add(TEntity entity)
at MultiContextConsoleApp.Program.Main(String[] args) in e:\shane\Projects\Orca\Sandbox\MultiContextConsoleApp\MultiContextConsoleApp\Program.cs:line 16
at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args)
at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args)
at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly()
at System.Threading.ThreadHelper.ThreadStart_Context(Object state)
at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)
at System.Threading.ThreadHelper.ThreadStart()
&lt;/code&gt;&lt;/pre&gt;

</description><pubDate>Tue, 02 Jun 2015 18:30:00 Z</pubDate><a10:updated>2015-06-02T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I came across the following exception whilst attempting working with a solution that contained a &lt;em&gt;couple of&lt;/em&gt; Entity Framework (EF) 6 database contexts.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;System.Data.Entity.Core.MetadataException

Schema specified is not valid. Errors: 
The mapping of CLR type to EDM type is ambiguous because multiple CLR types match the EDM type 'Setting'. Previously found CLR type 'SqlHelper.Primary.Setting', newly found CLR type 'SqlHelper.Secondary.Setting'.

at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.LoadAssemblyFromCache(Assembly assembly, Boolean loadReferencedAssemblies, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.ObjectItemCollection.ExplicitLoadFromAssembly(Assembly assembly, EdmItemCollection edmItemCollection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.ExplicitLoadFromAssembly(Assembly assembly, ObjectItemCollection collection, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly, Action`1 logLoadMessage)
at System.Data.Entity.Core.Metadata.Edm.MetadataWorkspace.LoadFromAssembly(Assembly assembly)
at System.Data.Entity.Internal.InternalContext.TryUpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.UpdateEntitySetMappingsForType(Type entityType)
at System.Data.Entity.Internal.InternalContext.GetEntitySetAndBaseTypeForType(Type entityType)
at System.Data.Entity.Internal.Linq.InternalSet`1.Initialize()
at System.Data.Entity.Internal.Linq.InternalSet`1.get_InternalContext()
at System.Data.Entity.Internal.Linq.InternalSet`1.ActOnSet(Action action, EntityState newState, Object entity, String methodName)
at System.Data.Entity.Internal.Linq.InternalSet`1.Add(Object entity)
at System.Data.Entity.DbSet`1.Add(TEntity entity)
at MultiContextConsoleApp.Program.Main(String[] args) in e:\shane\Projects\Orca\Sandbox\MultiContextConsoleApp\MultiContextConsoleApp\Program.cs:line 16
at System.AppDomain._nExecuteAssembly(RuntimeAssembly assembly, String[] args)
at System.AppDomain.ExecuteAssembly(String assemblyFile, Evidence assemblySecurity, String[] args)
at Microsoft.VisualStudio.HostingProcess.HostProc.RunUsersAssembly()
at System.Threading.ThreadHelper.ThreadStart_Context(Object state)
at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state, Boolean preserveSyncCtx)
at System.Threading.ExecutionContext.Run(ExecutionContext executionContext, ContextCallback callback, Object state)
at System.Threading.ThreadHelper.ThreadStart()
&lt;/code&gt;&lt;/pre&gt;

&lt;!--excerpt--&gt;

&lt;h2&gt;Reproducing the Problem&lt;/h2&gt;

&lt;p&gt;Here's a quick run down of the steps required to reproduce the scenario:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start off by creating two databases.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a new table with the same name on each of them.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/10_DatabaseSchema.png" alt="Database Schema" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, open up Visual Studio and create a Class Library project to host both EF database contexts.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Within the same project, create two EF (&lt;em&gt;ADO.NET Entity Data Model&lt;/em&gt;) contexts; one for each database. Ensure that each context is under its own namespace.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/15_InitialProjectStructure.png" alt="Project Structure" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/20_PrimaryContext.png" alt="Primary Context" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/30_SecondaryContext.png" alt="Secondary Context" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/40_PrimarySetting.png" alt="Primary Context Settings File" /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/50_SecondarySetting.png" alt="Secondary Context Settings File" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, let's create a simple console application that would act as the client and invoke operations on the contexts. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run the application - an exception is thrown the moment there is any kind of interaction with either of the contexts. &lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/60_Exception.png" alt="SSMS Context Menu" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;The Solution&lt;/h2&gt;

&lt;p&gt;This issue is only reproducible when working with multiple database contexts that have tables with the same name and share the same assembly. It does not even matter that the contexts are across different namespaces. &lt;/p&gt;

&lt;p&gt;The problem appears to be how Entity Framework resolves namespaces, &lt;a href="http://entityframework.codeplex.com/workitem/483"&gt;this ticket goes into more detail&lt;/a&gt;. I was able to reproduce this with the latest version of EF and a fix does not appear to be on the immediate schedule. &lt;/p&gt;

&lt;p&gt;Luckily there are couple of workarounds which are pretty straightforward:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Make sure that that both contexts don't share tables with the same name - &lt;em&gt;not the most practical approach&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Restructure the solution by isolating the database contexts within their own assemblies.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/EFNamespaceConflict/70_NewProjectStructure.png" alt="SSMS Context Menu" /&gt;   &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://entityframework.codeplex.com/workitem/483"&gt;CodePlex - Entity Framework - View Issue #483: Can't map two classes with same name from different namespace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/14927391/the-mapping-of-clr-type-to-edm-type-is-ambiguous-with-ef-6-5"&gt;StackOverflow.com - c# - The mapping of CLR type to EDM type is ambiguous with EF 6 &amp;amp; 5?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://social.msdn.microsoft.com/Forums/en-US/5a8ea003-c6bc-4fc6-ad2a-634f09447c49/ef4-mapping-of-clr-type-to-edm-type-is-ambiguous-error?forum=adodotnetentityframework"&gt;MSDN Forum - EF4 Mapping of CLR type to EDM type is ambiguous error.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/06/generate-clean-up-script-drop-objects-sql-database/</guid><link>http://nullfactory.net/2015/06/generate-clean-up-script-drop-objects-sql-database/</link><title>Generate a Clean Up Script to Drop All Objects in a SQL Server Database</title><description>&lt;p&gt;I needed a quick and reusable way to drop all SQL server objects from an Azure database. The objective was to have some kind of process to clean up and prep the database before the main deployment is kicked off. And given that I am particularly biased towards using a sql script my search for a solution focused around it.&lt;/p&gt;

&lt;p&gt;In addition to actually dropping the artifacts, the script should be aware of the order in which it should do it - that is to drop the most dependent objects first and work its way towards the least dependent ones. And my nice-to-have feature is to be able to parameterize the schema name so that it could be used with a multi-tenant database schema.&lt;/p&gt;

&lt;p&gt;I saw a few possible solutions and finally settled on using the &lt;a href="http://stackoverflow.com/questions/536350/drop-all-the-tables-stored-procedures-triggers-constraints-and-all-the-depend"&gt;out-of-the-box feature&lt;/a&gt; that's already available through SQL Server Management Studio (SSMS).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open up SQL Server Management Studio.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select &lt;code&gt;Task  &amp;gt; Generate Script...&lt;/code&gt; on on your the database context menu. This would open up the &lt;code&gt;Generate and Publish Scripts&lt;/code&gt; dialog.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/10_ContextMenu.png" alt="SSMS Context Menu" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, navigate to the &lt;code&gt;Choose Objects&lt;/code&gt; tab and select all the objects that need to be dropped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, on the &lt;code&gt;Set Scripting Options&lt;/code&gt; tab, select the preferred output location.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/20_SetScriptingOptions.png" alt="Set Scripting Options" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, click the &lt;code&gt;Advanced&lt;/code&gt; button which result in the &lt;code&gt;Advanced Scripting Options&lt;/code&gt; dialog.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/30_AdvancedScriptingOptions.png" alt="Advanced Scripting Options" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigate down towards to and change the &lt;code&gt;General &amp;gt; Script DROP and CREATE&lt;/code&gt; option to &lt;code&gt;Script DROP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Set the default values for the rest of the steps and finally click the &lt;code&gt;Finish&lt;/code&gt; button.&lt;/li&gt;
&lt;/ol&gt;

</description><pubDate>Mon, 01 Jun 2015 18:30:00 Z</pubDate><a10:updated>2015-06-01T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I needed a quick and reusable way to drop all SQL server objects from an Azure database. The objective was to have some kind of process to clean up and prep the database before the main deployment is kicked off. And given that I am particularly biased towards using a sql script my search for a solution focused around it.&lt;/p&gt;

&lt;p&gt;In addition to actually dropping the artifacts, the script should be aware of the order in which it should do it - that is to drop the most dependent objects first and work its way towards the least dependent ones. And my nice-to-have feature is to be able to parameterize the schema name so that it could be used with a multi-tenant database schema.&lt;/p&gt;

&lt;p&gt;I saw a few possible solutions and finally settled on using the &lt;a href="http://stackoverflow.com/questions/536350/drop-all-the-tables-stored-procedures-triggers-constraints-and-all-the-depend"&gt;out-of-the-box feature&lt;/a&gt; that's already available through SQL Server Management Studio (SSMS).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Open up SQL Server Management Studio.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Select &lt;code&gt;Task  &amp;gt; Generate Script...&lt;/code&gt; on on your the database context menu. This would open up the &lt;code&gt;Generate and Publish Scripts&lt;/code&gt; dialog.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/10_ContextMenu.png" alt="SSMS Context Menu" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;First, navigate to the &lt;code&gt;Choose Objects&lt;/code&gt; tab and select all the objects that need to be dropped.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, on the &lt;code&gt;Set Scripting Options&lt;/code&gt; tab, select the preferred output location.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/20_SetScriptingOptions.png" alt="Set Scripting Options" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, click the &lt;code&gt;Advanced&lt;/code&gt; button which result in the &lt;code&gt;Advanced Scripting Options&lt;/code&gt; dialog.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/GenerateDropScript/30_AdvancedScriptingOptions.png" alt="Advanced Scripting Options" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Navigate down towards to and change the &lt;code&gt;General &amp;gt; Script DROP and CREATE&lt;/code&gt; option to &lt;code&gt;Script DROP&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Set the default values for the rest of the steps and finally click the &lt;code&gt;Finish&lt;/code&gt; button.&lt;/li&gt;
&lt;/ol&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;SSMS sorts out the dependencies and generates a script similar to the one below. Note that the statements are in the required sequence.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;USE [test]
GO
ALTER TABLE [dbo].[Contact] DROP CONSTRAINT [FK_Contact_Company]
GO
/****** Object:  Table [dbo].[Contact]Script Date: 6/2/2015 9:33:36 AM ******/
DROP TABLE [dbo].[Contact]
GO
/****** Object:  Table [dbo].[Company]Script Date: 6/2/2015 9:33:36 AM ******/
DROP TABLE [dbo].[Company]
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I finally made the following tweaks to convert the script to a &lt;code&gt;SQLCMD&lt;/code&gt; script and parameterize the schema:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;:setvar TenantSchemaName "scm"

ALTER TABLE [$(TenantSchemaName)].[Contact] DROP CONSTRAINT [FK_Contact_Company]
GO
/****** Object:  Table [$(TenantSchemaName)].[Contact]    Script Date: 6/2/2015 9:33:36 AM ******/
DROP TABLE [$(TenantSchemaName)].[Contact]
GO
/****** Object:  Table [$(TenantSchemaName)].[Company]    Script Date: 6/2/2015 9:33:36 AM ******/
DROP TABLE [$(TenantSchemaName)].[Company]
GO
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Although, I was not really focused on automation, it should not be too difficult to integrate it with existing automated processes.&lt;/p&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/536350/drop-all-the-tables-stored-procedures-triggers-constraints-and-all-the-depend"&gt;StackOverflow.com - sql server - Drop all the tables, stored procedures, triggers, constraints and all the dependencies in one sql statement - Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item><item><guid isPermaLink="true">http://nullfactory.net/2015/05/recording-diagnostics-azure-app-service-website-log4net/</guid><link>http://nullfactory.net/2015/05/recording-diagnostics-azure-app-service-website-log4net/</link><title>Recording Diagnostics on a Azure App Service Hosted Website using Log4Net</title><description>&lt;p&gt;I've been working on moving an existing web based software solution into the Azure cloud ecosystem. The solution is tightly integrated with and uses Log4Net as it logging framework. My primary goal, in terms of logging, was to keep as much of my original architecture intact and at the same time make maximum use of the diagnostics infrastructure that is available in Azure.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://azure.microsoft.com/en-in/documentation/articles/web-sites-enable-diagnostic-log/"&gt;official documentation states&lt;/a&gt; that calls to the &lt;code&gt;System.Diagnostics.Trace&lt;/code&gt; methods are all that is required to start capturing diagnostic information. In summary, this is all I needed to do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Enable diagnostics and configure the storage locations (discussed later down the post).&lt;/li&gt;
&lt;li&gt;From within my code write the &lt;code&gt;Warning&lt;/code&gt;, &lt;code&gt;Error&lt;/code&gt; and &lt;code&gt;Information&lt;/code&gt; messages via their respective trace methods.&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;Azure starts capturing the custom diagnostics information - PROFIT! &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sounds simple enough. &lt;/p&gt;

&lt;p&gt;So I thought if I just set up a &lt;code&gt;TraceAppender&lt;/code&gt; everything would work fine and that would be the end of it. The results were not what I was expecting and this was the output in my table storage:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/10_AppServiceTableDiag.png" alt="Table Diagnostics" /&gt;&lt;/p&gt;

&lt;p&gt;The trace entries are bunched together as a single &lt;code&gt;Verbose&lt;/code&gt; entry and the writes appear to be buffered. Not acceptable. I suppose the buffering could be because I had not used the &lt;code&gt;ImmediateFlush&lt;/code&gt; option for the &lt;code&gt;TraceAppender&lt;/code&gt;, but I need to have each Trace statement to have its own entry in the table.&lt;/p&gt;

&lt;p&gt;While there are a lot of posts on the internet on how to setup Log4Net with Azure, most of them appear to be out of date and seem to be compensating for features were not available in the Azure at the time of their implementation. Then there are others that targeted towards integrating with the Cloud Service which is not what I was looking for.&lt;/p&gt;

</description><pubDate>Sat, 23 May 2015 18:30:00 Z</pubDate><a10:updated>2015-05-23T18:30:00Z</a10:updated><a10:content type="html">&lt;p&gt;I've been working on moving an existing web based software solution into the Azure cloud ecosystem. The solution is tightly integrated with and uses Log4Net as it logging framework. My primary goal, in terms of logging, was to keep as much of my original architecture intact and at the same time make maximum use of the diagnostics infrastructure that is available in Azure.&lt;/p&gt;

&lt;p&gt;The &lt;a href="http://azure.microsoft.com/en-in/documentation/articles/web-sites-enable-diagnostic-log/"&gt;official documentation states&lt;/a&gt; that calls to the &lt;code&gt;System.Diagnostics.Trace&lt;/code&gt; methods are all that is required to start capturing diagnostic information. In summary, this is all I needed to do:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Enable diagnostics and configure the storage locations (discussed later down the post).&lt;/li&gt;
&lt;li&gt;From within my code write the &lt;code&gt;Warning&lt;/code&gt;, &lt;code&gt;Error&lt;/code&gt; and &lt;code&gt;Information&lt;/code&gt; messages via their respective trace methods.&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;li&gt;Azure starts capturing the custom diagnostics information - PROFIT! &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Sounds simple enough. &lt;/p&gt;

&lt;p&gt;So I thought if I just set up a &lt;code&gt;TraceAppender&lt;/code&gt; everything would work fine and that would be the end of it. The results were not what I was expecting and this was the output in my table storage:&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/10_AppServiceTableDiag.png" alt="Table Diagnostics" /&gt;&lt;/p&gt;

&lt;p&gt;The trace entries are bunched together as a single &lt;code&gt;Verbose&lt;/code&gt; entry and the writes appear to be buffered. Not acceptable. I suppose the buffering could be because I had not used the &lt;code&gt;ImmediateFlush&lt;/code&gt; option for the &lt;code&gt;TraceAppender&lt;/code&gt;, but I need to have each Trace statement to have its own entry in the table.&lt;/p&gt;

&lt;p&gt;While there are a lot of posts on the internet on how to setup Log4Net with Azure, most of them appear to be out of date and seem to be compensating for features were not available in the Azure at the time of their implementation. Then there are others that targeted towards integrating with the Cloud Service which is not what I was looking for.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;p&gt;Unable to find any existing implementation, I decided to roll my own appender. I started off by &lt;a href="https://github.com/apache/log4net/blob/trunk/src/log4net/Appender/TraceAppender.cs"&gt;looking at the TraceAppender code&lt;/a&gt; and observed that it just does a &lt;code&gt;Trace.Write&lt;/code&gt;, which I suppose is why Azure categorizes the entries under the &lt;code&gt;Verbose&lt;/code&gt; level. So my implementation would be identical but would make explicit calls to the &lt;code&gt;Trace.TraceWarning&lt;/code&gt;, &lt;code&gt;Trace.TraceError&lt;/code&gt; and &lt;code&gt;Trace.TraceInformation&lt;/code&gt; methods as necessary.&lt;/p&gt;

&lt;p&gt;Its quite simple really, this is the gist of it:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protected override void Append(LoggingEvent loggingEvent)
{
    string logMessage = string.Format(this.RenderLoggingEvent(loggingEvent), ((LayoutSkeleton)this.category).Format(loggingEvent));

    if (loggingEvent.Level == Level.Alert ||
        loggingEvent.Level == Level.Critical ||
        loggingEvent.Level == Level.Emergency ||
        loggingEvent.Level == Level.Error ||
        loggingEvent.Level == Level.Fatal ||
        loggingEvent.Level == Level.Log4Net_Debug ||
        loggingEvent.Level == Level.Severe)
    {
        Trace.TraceError(logMessage);
    }
    else if (loggingEvent.Level == Level.Warn)
    {
        Trace.TraceWarning(logMessage);
    }
    else if (loggingEvent.Level == Level.Debug ||
        loggingEvent.Level == Level.Fine ||
        loggingEvent.Level == Level.Finer ||
        loggingEvent.Level == Level.Finest ||
        loggingEvent.Level == Level.Info ||
        loggingEvent.Level == Level.Notice ||
        loggingEvent.Level == Level.Trace ||
        loggingEvent.Level == Level.Verbose)
    {
        Trace.TraceInformation(logMessage);
    }

    if (!this.ImmediateFlush)
    {
        return;
    }

    Trace.Flush();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Configure the appender the same as you would the &lt;code&gt;TraceAppender&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;appender name="SimpleTraceAppender" type="Nullfactory.Log4Net.SimpleTraceAppender, Nullfactory.Log4Net"&amp;gt;
    &amp;lt;layout type="log4net.Layout.PatternLayout"&amp;gt;
        &amp;lt;conversionPattern value="%date [%thread] %-5level %logger [%property{NDC}] - %message%newline" /&amp;gt;
    &amp;lt;/layout&amp;gt;
&amp;lt;/appender&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And finally for the webjob contained in my solution, I paired my custom appender with a &lt;code&gt;ConsoleAppender&lt;/code&gt; so that it would automatically take advantage of the web job logging architecture without any additional code.&lt;/p&gt;

&lt;p&gt;I've uploaded my &lt;a href="https://github.com/shanec-/Nullfactory-Azure"&gt;full implementation and sample usage here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;Enabling Diagnostics on the Azure Portal&lt;/h2&gt;

&lt;p&gt;There are couple of steps that need to be done in order to enable diagnostics on the app service:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Login to the older Azure portal - &lt;a href="https://manage.windowsazure.net"&gt;https://manage.windowsazure.net&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ensure that you already have a storage account set up. This would house the table that stores the diagnostic entries. &lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next, navigate to the web app configuration page. &lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/20_WebAppDiagConfig.png" alt="Web App Configure" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enable application logging (table storage)
and select a logging level.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/30_DiagTableStorage.png" alt="Web App, Application Diagnostics" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Next click on the &lt;code&gt;manage table storage&lt;/code&gt; button allows to configure the storage account and the table into which the diagnostic information is recorded into.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/40_DiagTableStorage2.png" alt="Web App Application Diagnostics Table Name" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;That's it, the web app should start capturing logging information. &lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There were a few quirks that I faced attempting to set it up:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This how-to provides the following instructions on &lt;a href="http://azure.microsoft.com/en-us/documentation/articles/web-sites-enable-diagnostic-log/"&gt;enabling diagnostics&lt;/a&gt;:  &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;When enabling Application Logging you must also select the Logging Level and whether to enable logging to the file system, table storage, or blob storage. While all three storage locations provide the same basic information for logged events, table storage and blob storage log additional information such as the instance ID, thread ID, and a more granular timestamp (tick format) than logging to file system.&lt;/p&gt;
  
  &lt;p&gt;When enabling site diagnostics, you must select storage or file system for web server logging. Selecting storage allows you to select a storage account, and then a blob container that the logs will be written to. All other logs for site diagnostics are written to the file system only.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At the time of writing this post the preview portal does not appear to have the diagnostics feature fully ported. While it does have the switch to enable it, it does not provide the options to select the storage location. That's why it is required to be enabled via the &lt;a href="https://manage.windowsazure.com/"&gt;older portal&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/60_PreviewAppDiagLogs.png" alt="Preview Portal, Web App Logging" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The newer (v2) storage accounts do not seem to be recognized by the older portal. So if you already using them via the preview portal, you would need to create a second set of storage accounts just for the diagnostics. Again, this is something I think this would be sorted out in future releases  &lt;/p&gt;

&lt;p&gt;&lt;img src="http://nullfactory.net/images/posts/AzureAppSvcDiag/50_StorAccV2Diag.png" alt="Web App, Diagnostics Storage Account V2 Missing" /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;References&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/log4net/blob/trunk/src/log4net/Appender/TraceAppender.cs"&gt;log4net/TraceAppender.cs at trunk Â· apache/log4net Â· GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://azure.microsoft.com/en-in/documentation/articles/web-sites-enable-diagnostic-log/"&gt;Enable diagnostics logging for web apps in Azure App Service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.tylerdoerksen.com/2012/04/15/logging-in-azure-part-1/"&gt;Logging in Azure: Part 1 | Tyler's Azure Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.tylerdoerksen.com/2012/04/17/logging-in-azure-part-2table-storage/"&gt;Logging in Azure: Part 2âTable Storage | Tyler's Azure Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zacg.github.io/blog/2014/02/05/azure-log4net-appender/"&gt;Azure Log4Net Appender - Zac Gross&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.kloud.com.au/2014/10/22/logging-with-log4net-and-azure-diagnostics-on-web-and-worker-roles/"&gt;Logging with log4net and Azure Diagnostics on Web and Worker Roles | Kloud Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://cloudshaper.wordpress.com/2010/10/30/logging-with-log4net-on-the-azure-platform/"&gt;Logging with Log4Net on the Azure platform | Cloud Shaper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</a10:content></item></channel></rss>